Ethics in Technical Communication
=================================

Dr. Murray's Notes on Writing Ethically & Effectively
-----------------------------------------------------

### Examples of Unethical Communication

An integral part of maintaining ethical communication pertains to
recognizing the ways ways in which "colleagues or supervisors on the job
might violate or disregard standards of practice" (Tabeaux and Dragga
p. 36)

Here are examples of unethical writing (Don't do these)

1.  Plagiarism and theft of intellectual property
2.  Deliberately imprecise or ambiguous language
3.  Manipulation of numerical information
4.  Use of misleading illustrations
5.  Promotion of prejudice
6.  Failing to make information accessible
7.  Uncritical use of information

### Ethics Decision Checklist

Here are some good questions to ask yourself

-   What is the nature of the ethical dilemma?
-   What are the sepecific aspects of this dilemma that make you
    uncomfortable?
-   What are your competing obligations in this dilemma?
-   What advice does a trusted supervisor/mentor offer?
-   Does your company's code of conduct address this issue
-   Does your professional association's code of conduct address this
    issue?
-   What are you unwilling to do? What are you willing to do?
-   How will you explain or justify your deicion(s)?

### Good Writing = Ethical Writing

> "If you want your report to be read, use a style that your readers can
> follow easily. If your readers can't understand your report as they
> read it and must constantly reread sentences and paragraphs, they may
> just disregard it or toss it out. Unreadable documents usually result
> from ineffective style" (Tabeaux and Dragga p. 50).

### Effective Paragraphs

1.  Begin each paragraph with a topic sentence that summarizes the ideas
    to come
2.  Include only information relevant to the topic sentence
3.  Place sentences in logical order
4.  Avoid long paragraphs (don't exceed 5-7 sentences, or one page)

### Effective writing style

1.  Determining the reader's knowledge of the subject
2.  Determining whether a particular style will be expected
3.  Adjusting the style to the readers, purpose, and context

### Effective sentences

1.  Watch sentence length
2.  Keep subjects and verbs close together
3.  Avoid pompous language ("write to express, not to impress")
4.  Avoid excessive use of is/are verbs, use other verbs for variety
5.  Use active voice for clarity (as opposed to passive voice)
    1.  Caveat: Some disciplines use passive voice in the academy

### Style Checklist

-   Planning
    -   How will you adjust your writing style to acommodate your
        readers' knowledge of the subject?
    -   How will you meet your readers' expectations about style for the
        specific kind of document you are writing?
    -   Can you determine the appropriate reading level for the context
        in which the document will be read?
    -   How will you adjust your style so that it is appropriate for the
        professional relationship you have with your readers?
-   Revision
    -   Do paragraphs begin with a topic sentence? Do the subsequent
        sentences in the paragraph build on the idea in the topic
        sentence?
    -   Are most sentences twenty words or shorter? Could you make any
        longer sentences shorter?
    -   Are subjects and verbs close together in your sentences?
    -   Have you used specific nouns and concrete verbs?
    -   Have you avoided ponderous and impersonal language?
    -   Have you avoided is/are verbs when possible?
    -   Are most of your sentences written in active voice? Could you
        change any sentence with passive voice to active voice?
    -   Have you defined everything that might require defining?
    -   Could you write any of your sentences with equal clarity but
        fewer words?

Introduction to Ethics
----------------------

Ethis is extremely important in technical communication.

Good writing is not just a matter of grammar or functionality. There is
also the potential to cause harm.

Also, legality and morality may overlap, but they do not define each
other.

### Four basic kinds of moral standards

According to ethicist Manuel G. Velasquez, there are four basic kinds of
moral standards:

1.  Utility - Benefits from taking an action
2.  Rights - Individual entitlements to autonomy and well-being
3.  Justice - Distribution of benefits and burdens among people
4.  Care - Compassion, concern, love, friendship, kindness

These standards can contradict each other, and the real world is often
quite messy. Though, after some analysis, you can likely come to a
reasonable decision.

Kant's maxim is to do whatever you would want everyone else to do in
this situation. If you don't want everyone stealing, then you should not
steal. If you want people to return lost items, then return lost items.


Characteristics of Technical Writing for Computer Science
=========================================================

There is only one document in this section

Characteristics of Technical Writing for Computer Science
---------------------------------------------------------

### Constructing Sentences

Sentences are the building blocks of writing.

-   Write in complete sentences, avoid sentence fragments and run-on
    sentences.
    -   Sentences should generally be between 15 and 20 words
-   Maintain subject-verb agreement
    -   If the noun is plural or singular, then the verb should match
-   Use parallel structures

### Writing Paragraphs

-   Two types: body paragraphs & transitional paragraphs
    -   Body paragraphs create the overall structure
    -   Transitional paragraphs help readers follow your argument
-   All paragraphs contain a topic sentence and supporting details
    -   The paragraph should remain focused on this topic
-   Effective paragraphs center around one key idea

### Transitions & Signposting

Using transitions and signposting helps readers understand where you are
going and follow your line of thought.

-   Transitions demonstrate the relationship between ideas
-   To show additions, use *likewise*, *similarly*, or *moreover*
-   To show contrast, use *although*, *however*, *in contrast*, or *on
    the other hand*
-   To show cause and effect, use *as a result*, *consequently*, or
    *therefore*
-   To show summary or conclusion, use *at last*, *finally*, *in
    conclusion*, or *in summary*
-   Signposting helps readers understand where your argument is going

### Tone, professionalism, and formality

-   Revisit your understanding of audience, purpose, and topic to assess
    the tone
-   Formality is audience-dependent
-   Subject also influences formality
    -   Some topics warrant being serious even if everyone involved is
        your friend
-   When in doubt, be formal and professional

### Word Choice

The goal is to be clear, specific, and concise. Write to express, not to
impress

-   Word choice can enhance or detract from your writing
-   Avoid unnecessary jargon and "fancy" (pompous) words
-   Beware of wordy phrases

### Consider Cultural Background

-   Many of your readers may not be native English speakers
-   Use short, clear sentences and active voice
-   Consider including a glossary of key terms (e.g. in an Appendix)

### Inclusive language

We write for people first

-   Person-first language
-   Makes your writing more accessible to a wide audience


Algorithmic Bias
================

Algorithmic Bias and Fairness
-----------------------------

Algorithms are created by people and trained on human data. As a result,
although the processes may be objective, the objectivity is compromised
by biased data.

We all have bias. It becomes an issue when it morphs into harmful
discrimination.

### 5 Types of biases to look out for

1.  Data reflects existing biases
2.  Unbalanced classes in training data
3.  Data doesn't capture the right value
4.  Data amplified by feedback loop
5.  Malicious data attack or manipulation

### Correlated Features

Even if data such as race is not collected, bias can still be introduced
due to **correlated features**. This is when data is unintentionally
correlated with other data.

**Example:** ZIP Codes can be correlated to race due to segregation.
Purchasing patterns can be correlated to gender. Sexual orientation is
also (apparently!) correlated with certain characteristics of a social
media profile photo.

### Unbalanced classes in training data

If an AI has a large set of training data for people of Group A, but not
much for Group B, then it will likely have faulty results when analyzing
members of Group B.

**Example:** Facial recognition software failing for nonwhite people

### Data doesn't capture the right value

Some things are impossible to quantify with a single number. This can
result in easily fooled AI systems that give ridiculous ratings based on
simple criteria.

**Example:** Good writing is hard to define, and an AI to detect good
writing could be fooled by another AI.

### Data amplified by feedback loop

**Example:** PREDPOL would send police to areas with a relatively high
amount of racial minorities. More arrests occur due to a higher police
presence, causing PREDPOL to send even more police.

### Malicious data attack or manipulation

**Example:** Microsoft's chat AI "Tay" was trained by users to be
sexist, racist, anti-semitic, etc.

### Conclusion

Because AI can clearly be manipulated or make mistakes, the output of AI
should be taken with a grain of salt. An AI system may be very good at
finding correlations, but it doesn't understand correlation vs
causation, and it can be coerced or compromised into acting biased.

Search Engine Breakdown
-----------------------

> Two leading researchers investigate racial bias built into widely used
> search engines.

Google handles over 90% of all information searches online globally.

In the past, content was not ranked, and instead ordered by subject
(like a library using the Dewey Decimal System). Google ranks content,
and isn't entirely clear on what causes content to be ranked above other
content.

Ranks imply that one result is objectively better than the other, and
doesn't acknowledge that everything is created in a specific cultural
context.

Google is effective when searching for specific, objective questions,
such as the locations of restaurants. Though, when more complex
questions without a single, objective answer arise, it falls flat.

White-associated names more often result in neutral ads on Google.
Black-associated names resulted in ads suggesting an arrest record more
than 80% of the time (regardless if anyone with that name has been
arrested before). This places an unfair disadvantage for people who have
black-associated names.

Bias can enter the system through common members of society society
clicking on links and giving those links more weight.

When you type in a query to Google, you don't necessarily get a
counterpoint or something to reframe what you're thinking.

Coded Bias
----------

> Coded Bias follows M.I.T. Media Lab computer scientist Joy Buolamwini,
> along with data scientists, mathematicians, and watchdog groups from
> all over the world, as they fight to expose the discrimination within
> facial recognition algorithms now prevalent across all spheres of
> daily life.

### 5 Sentence Summary

AI algorithms can unfairly harm or track people without any oversight.
Some police, corporations, and apartments are already using AI
algorithms in spite of its potential for error and potential to infringe
on legal rights. These algorithms' impact on human beings is often
overlooked and understated, and there are already instances of it being
used explicitly to do harm. Technology is advancing faster than it is
being regulated or inspected. The potential for abuse and discrimination
is too great to ignore.

An Algorithm That Predicts Deadly Infections Is Often Flawed
------------------------------------------------------------

> A study found that a system used to identify cases of sepsis missed
> most instances and frequently issued false alarms.

> A complication of infection known as **sepsis** is the number one
> killer in US hospitals

Epic Systems created a system to predict cases of sepsis based on a
patient's test results. It looked for signs of sepsis using a
proprietary formula. It turns out that it is deeply flawed. It missed
2/3 of each sepsis case, most cases that it did find were also found by
human staff, and it frequently issued false alarms.

Other systems have been used in other areas, but also performed poorly
or did not perform as well for nonwhite patients.

Vendors don't disclose how these systems work behind the scenes, and
there are not any independent validations of the vendors' claims.

An excess of false alarms and bad alerts can cause "alert fatigue",
which is when you feel overwhelmed and stop caring about each individual
popup regardless of its importance.

Alondra Nelson Wants to Make Science and Tech More Just
-------------------------------------------------------

> The deputy director of the White House science office plans to tackle
> algorithmic bias and start candid conversations about the past

I decided to pick out some excerpts that I feel drive the point home
well enough. The bold text is something I added.

> In her first formal remarks in her new role in January, **Nelson
> called science a social phenomenon** and said technology such as
> artificial intelligence can reveal or reflect dangerous social
> architectures that undergird the pursuit of scientific progress. In an
> interview with WIRED, **Nelson said the Black community in particular
> is overexposed to the harms of science and technology and is
> underserved by the benefits.**

> "I think it takes a real commitment to want to move from technical
> standards to what I call sociotechnical standards. **A tool can be
> technically exactly right**---we can think about the work that Joy
> Buolamwini and Timnit Gebru did around bias and facial recognition
> \[which showed that many facial recognition programs are better at
> identifying white and male faces than female faces with dark skin\].
> **But that doesn't deal with the socio-technical issue, which is that
> there's still potential for disproportionate harm based on the
> incompleteness of the database** and what we think the data tells us
> and what we think it"predicts\" in the world.\"

> "Science, like democracy, is a process, and it's never quite realized
> but always trying to perfect itself."

AI Is Biased. Here's How Scientists Are Trying to Fix It
--------------------------------------------------------

> Researchers are revising the ImageNet data set. But algorithmic
> anti-bias training is harder than it seems.

An AI algorithm called ImageNet was made that could identify images
based on labels that humans gave them. However this resulted in some
things being identified as slurs or other offensive phrases.

Although the algorithm is technically exact, the human bias and
stereotypes found its way into the dataset, meaning the algorithm's
objectivity was compromised.

Scientists countered this by creating other machines that could identify
human prejudice and remove it from the data set. This is still
imperfect, as it is still a human-made algorithm, but it is better than
nothing.

They look for terms that project meaning, like "philanthropist", and try
to remove those. This (in theory) results in only objective, fair terms
being left behind.

> "Debiasing humans is harder than debiasing AI systems" - Olga
> Russakovsky, Princeton

A British AI Tool to Predict Violent Crime Is Too Flawed to Use
---------------------------------------------------------------

> A government funded system known as Most Serious Violence was built to
> predict first offenses but turned out to be wildly inaccurate

Most Serious Violence (MSV) was made to predict if someone would commit
their first violent offense with a gun or a knife in the next two years.
It scored people, with higher scores implying a higher likelihood to
commit violent crime.

A coding "flaw" was discovered that made MSV unviable. Before that flaw
was discovered, MSV was thought to be extremely accurate. Now, even in
the ideal conditions with the base case scenario, MSV is only 51%
accurate at its absolute best.

MSV won't be used any time soon.

> "Rare events are much harder to predict than common events," says
> Melissa Hamilton, a reader in law and criminal justice at the
> University of Surrey

Lawmakers Demand Scrutiny of Racial Bias in Health Algorithms
-------------------------------------------------------------

> Four Congress members say formulas that include race as a factor can
> hurt Black Americans' access to care

A group of Black former NFL players filed a lawsuit claiming that the
NFL's system for compensation for brain injuries was racially biased due
to an algorithm that assumes white people have "intrinsically higher
cognitive function than Black people"

Now Four Democratic lawmakers warn that including race in
healthcare-related calculations can cause doctors to make biased
decisions against people of color.

> "Black patients are effectively required to get sicker than white
> patients before they can access specialist care" (Nwamaka Eneanya)

> "We need to be more responsible and stop putting social constructs
> into these tools." (Nwamaka Eneanya)


